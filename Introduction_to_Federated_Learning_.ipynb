{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Introduction to Federated Learning .ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soumyadip1995/Federated-Learning/blob/master/Introduction_to_Federated_Learning_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "j1LLbmEbKCkc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Introduction To Federated Learning (Continued)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##Federated Learning\n",
        "\n",
        "In the previous post, we have been introduced to federated learning. Let's go deeper \n",
        "\n",
        "To avoid collecting the training data, a distributed way of running the\n",
        "learning algorithm is required as mentioned in the previous blog post. The parts of the algorithm that directly make use of the data need to be executed on the users’ computers. These\n",
        "correspond to the sections of the algorithm that compute the previously\n",
        "mentioned updates. In Federated Learning, users compute updates based\n",
        "on their locally available training data and send them to a server. These\n",
        "updates are much harder to interpret than pure data, so this is a major\n",
        "improvement for privacy. For some applications with huge amounts of data,\n",
        "it might also be cheaper to communicate the updates compared to directly\n",
        "sending the data. While the computers of users generally have much less computational\n",
        "power than servers in a data center, there is also less data on them. By having a lot of users, the computations that need to be performed are vastly distributed. There is not much work to do on the computer of each individual.\n",
        "\n",
        "\n",
        "\n",
        "##Optimization\n",
        "\n",
        "\n",
        "Conventional machine learning can be seen as a centralized system where all the work is performed on one server. In the process described so far, responsibilities are moved from the server to the clients. This is not a fully decentralized system because the server still runs a part of the algorithm. Instead, it is a federated system: A federation of clients takes over a significant amount of work but there is still one central entity, a server, coordinating everything. Before the server starts off the distributed learning process, it needs to initialize the model. Theoretically, this can be done randomly. In practice,\n",
        "it makes sense to smartly initialize the model with sensible default values.\n",
        "If some data is already available on the server, it can be used to pretrain\n",
        "the model. In other cases, there might be a known configuration of model\n",
        "parameters that already leads to acceptable results. Having a good first\n",
        "model gives the training process a headstart and can reduce the time it\n",
        "takes until convergence. (Img: Florian)\n",
        "\n",
        "![alt text](https://florian.github.io/assets/posts/federated-learning/iteration.png)\n",
        "\n",
        "After the model has been initialized, the iterative training process is\n",
        "kicked off. A visualization of the steps performed in each iteration is shown\n",
        "in Figure. At the beginning of an iteration, a subset of *K* clients are\n",
        "randomly selected by the server. They receive a copy of the current model\n",
        "parameters θ and use their locally available training data to compute an\n",
        "update. The update of the i-th client is denoted by $Hi$.The updates are then sent back to the server.\n",
        "\n",
        " we are  assuming $θ$ and $H_i$ to be vectors. However, the same concepts transfer directly to any sequence of vectors since they can be concatenated into one long vector. The server waits until it has received all updates and then combines them into one final update. This is usually done by computing an average of all updates, weighted by how many training examples the respective clients."
      ]
    },
    {
      "metadata": {
        "id": "81X6TzOq9U5D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Federated Learning Example\n",
        "Let's start by training a toy model the centralized way. This is about a simple as models get. We first need:\n",
        "\n",
        "a toy dataset\n",
        "a model\n",
        "some basic training logic for training a model to fit the data.\n",
        "Note: If this API is un-familiar to you - head on over to [fast.ai](http://fast.ai/ ) and take their course before continuing in this tutorial."
      ]
    },
    {
      "metadata": {
        "id": "UqV34YvbQ7hk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Let's take a look at some code !!\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "GSTJfCed9dTh",
        "colab_type": "code",
        "outputId": "315101b8-89e0-450a-c695-8af695be4499",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "\n",
        "# A Toy Dataset\n",
        "data = torch.tensor([[0,0],[0,1],[1,0],[1,1.]], requires_grad=True)\n",
        "target = torch.tensor([[0],[0],[1],[1.]], requires_grad=True)\n",
        "\n",
        "# A Toy Model\n",
        "model = nn.Linear(2,1)\n",
        "\n",
        "def train():\n",
        "    # Training Logic\n",
        "    opt = optim.SGD(params=model.parameters(),lr=0.1)\n",
        "    for iter in range(20):\n",
        "\n",
        "        # 1) erase previous gradients (if they exist)\n",
        "        opt.zero_grad()\n",
        "\n",
        "        # 2) make a prediction\n",
        "        pred = model(data)\n",
        "\n",
        "        # 3) calculate how much we missed\n",
        "        loss = ((pred - target)**2).sum()\n",
        "\n",
        "        # 4) figure out which weights caused us to miss\n",
        "        loss.backward()\n",
        "\n",
        "        # 5) change those weights\n",
        "        opt.step()\n",
        "\n",
        "        # 6) print our progress\n",
        "        print(loss.data)\n",
        "\n",
        "train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(5.8433)\n",
            "tensor(0.6510)\n",
            "tensor(0.2085)\n",
            "tensor(0.1366)\n",
            "tensor(0.1020)\n",
            "tensor(0.0774)\n",
            "tensor(0.0588)\n",
            "tensor(0.0448)\n",
            "tensor(0.0341)\n",
            "tensor(0.0260)\n",
            "tensor(0.0198)\n",
            "tensor(0.0151)\n",
            "tensor(0.0115)\n",
            "tensor(0.0088)\n",
            "tensor(0.0067)\n",
            "tensor(0.0051)\n",
            "tensor(0.0039)\n",
            "tensor(0.0030)\n",
            "tensor(0.0023)\n",
            "tensor(0.0018)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fkDjs_rH9oYJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "And there you have it! We've trained a basic model in the conventional manner. All our data is aggregated into our local machine and we can use it to make updates to our model. Federated Learning, however, doesn't work this way. So, let's modify this example to do it the Federated Learning way!\n",
        "\n",
        "So, what do we need:\n",
        "\n",
        "create a couple workers get pointers to training data on each worker updated training logic to do federated learning\n",
        "\n",
        "New Training Steps:\n",
        "\n",
        "send model to correct worker\n",
        "train on the data located there\n",
        "get the model back and repeat with next worker"
      ]
    },
    {
      "metadata": {
        "id": "cYG2Ga4w9yjZ",
        "colab_type": "code",
        "outputId": "1d538300-6e54-422e-cbe6-3803d5220cf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        }
      },
      "cell_type": "code",
      "source": [
        "import syft as sy\n",
        "hook = sy.TorchHook(torch)\n",
        "from syft import optim"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Torch was already hooked... skipping hooking process\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-24b289d22784>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msyft\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTorchHook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msyft\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'optim'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "DPClBHee-_Pz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create a couple workers\n",
        "\n",
        "bob = sy.VirtualWorker(hook, id=\"bob\")\n",
        "alice = sy.VirtualWorker(hook, id=\"alice\")\n",
        "\n",
        "# A Toy Dataset\n",
        "data = torch.tensor([[0,0],[0,1],[1,0],[1,1.]], requires_grad=True)\n",
        "target = torch.tensor([[0],[0],[1],[1.]], requires_grad=True)\n",
        "\n",
        "# get pointers to training data on each worker by\n",
        "# sending some training data to bob and alice\n",
        "data_bob = data[0:2]\n",
        "target_bob = target[0:2]\n",
        "\n",
        "data_alice = data[2:]\n",
        "target_alice = target[2:]\n",
        "\n",
        "# Iniitalize A Toy Model\n",
        "model = nn.Linear(2,1)\n",
        "\n",
        "data_bob = data_bob.send(bob)\n",
        "data_alice = data_alice.send(alice)\n",
        "target_bob = target_bob.send(bob)\n",
        "target_alice = target_alice.send(alice)\n",
        "\n",
        "# organize pointers into a list\n",
        "datasets = [(data_bob,target_bob),(data_alice,target_alice)]\n",
        "\n",
        "opt = optim.SGD(params=model.parameters(),lr=0.1)\n",
        "\n",
        "def train():\n",
        "    # Training Logic\n",
        "    opt = optim.SGD(params=model.parameters(),lr=0.1)\n",
        "    for iter in range(10):\n",
        "        \n",
        "        # NEW) iterate through each worker's dataset\n",
        "        for data,target in datasets:\n",
        "            \n",
        "            # NEW) send model to correct worker\n",
        "            model.send(data.location)\n",
        "\n",
        "            # 1) erase previous gradients (if they exist)\n",
        "            opt.zero_grad()\n",
        "\n",
        "            # 2) make a prediction\n",
        "            pred = model(data)\n",
        "\n",
        "            # 3) calculate how much we missed\n",
        "            loss = ((pred - target)**2).sum()\n",
        "\n",
        "            # 4) figure out which weights caused us to miss\n",
        "            loss.backward()\n",
        "\n",
        "            # 5) change those weights\n",
        "            opt.step(data.shape[0])\n",
        "            \n",
        "            # NEW) get model (with gradients)\n",
        "            model.get()\n",
        "\n",
        "            # 6) print our progress\n",
        "            print(loss.get()) # NEW) slight edit... need to call .get() on loss\\\n",
        "    \n",
        "# federated averaging"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n0MG_8n8926Y",
        "colab_type": "code",
        "outputId": "8587c3bb-8160-4b04-e9ff-cb50b48505cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        }
      },
      "cell_type": "code",
      "source": [
        "# create a couple workers\n",
        "\n",
        "\n",
        "\n",
        "train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.0013)\n",
            "tensor(0.0010)\n",
            "tensor(0.0008)\n",
            "tensor(0.0006)\n",
            "tensor(0.0005)\n",
            "tensor(0.0003)\n",
            "tensor(0.0003)\n",
            "tensor(0.0002)\n",
            "tensor(0.0002)\n",
            "tensor(0.0001)\n",
            "tensor(9.1281e-05)\n",
            "tensor(6.9796e-05)\n",
            "tensor(5.3368e-05)\n",
            "tensor(4.0807e-05)\n",
            "tensor(3.1203e-05)\n",
            "tensor(2.3859e-05)\n",
            "tensor(1.8244e-05)\n",
            "tensor(1.3950e-05)\n",
            "tensor(1.0667e-05)\n",
            "tensor(8.1569e-06)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Yaf1yrsu_Hkp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###cool!\n",
        "\n",
        "And voilà! We now are training a very simple Deep Learning model using Federated Learning! We send the model to each worker, generate a new gradient, and then bring the gradient back to our local server where we update our global model. Never in this process do we ever see or request access to the underlying training data! We preserve the privacy of Bob and Alice!!!\n",
        "\n",
        "###Shortcomings of this Example\n",
        "\n",
        "So, while this example is a nice introduction to Federated Learning, it still has some major shortcomings. Most notably, when we call model.get() and receive the updated model from Bob or Alice, we can actually learn a lot about Bob and Alice's training data by looking at their gradients. In some cases, we can restore their training data perfectly!\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "I5LWhNXz_rFz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##A few Key Properties of Federated Learning\n",
        "\n",
        "\n",
        "However, Federated Learning is a vastly more distributed way of collaboratively training machine learning models. It can be distinguished by several key properties. These also describe the some of the challenges in\n",
        "Federated Learning:\n",
        "\n",
        "\n",
        "\n",
        "**1. A huge number of users**: In a data center, there might be thousands\n",
        "of compute nodes. Popular consumer software has several orders of\n",
        "magnitude more users than that. All of these users should be able to\n",
        "participate in training the model at some point, so Federated Learning\n",
        "needs to scale to millions of users.\n",
        "\n",
        "**2. Unbalanced number of data points**: It is easy to guarantee that\n",
        "compute nodes have a similar number of data points in a data center.\n",
        "In Federated Learning, there is no control over the location of data at\n",
        "all. It is likely that some users generate vastly more data than others.\n",
        "\n",
        "**3. Different data distributions**: Even worse, no assumptions about\n",
        "the data distributions themselves can be made. While some users\n",
        "probably generate similar data, two randomly picked users are likely\n",
        "to compute very different updates. \n",
        "\n",
        "**4. Slow communication**: Since compute nodes in Federated Learning correspond to users’ computers, the network connections are often bad. This is especially the case if the training happens on mobile phones . Updates for complex models can be large, so this is problematic when training more sophisticated models\n",
        "\n",
        "**5. Unstable communication**: Some clients might not even be connected to the internet at all when the server asks them to send back model updates. In a data center, it is much easier to guarantee that\n",
        "compute nodes stay online.In a nutshell, Federated Learning is a massively distributed way of training machine learning models where very little control over the compute nodes and the distribution of data can be exercised. \n",
        "\n",
        "## Applications\n",
        "\n",
        "The protocol introduced so far is fairly abstract and it remains to be discussed what exactly can be implemented with it. In general, it is possible to use Federated Learning with any type of models for which some notion of updates can be defined. It turns out that most popular learning algorithms\n",
        "can be described in that way.\n",
        "\n",
        "\n",
        "Some algorithms, however, can not be reformulated for Federated Learn-\n",
        "ing. For example, k-NN requires memorizing the data points themselves which is not possible here. Non-parametric models in general can be problematic since their configurations often heavily depend on the exact data that was used to train them.\n",
        "\n",
        "In terms of data, Federated Learning is especially useful in situations where users generate and label data themselves implicitly. This is the case for the application of trying to predict the next word. The model can then automatically update itself without having to store the data permanently. In such a situation, Federated Learning is extremely powerful because models can be trained with a huge amount of data that is not stored and not directly shared with a server at all. We can thus make use of a lot of data that we could otherwise not have used without violating the users’ privacy."
      ]
    }
  ]
}